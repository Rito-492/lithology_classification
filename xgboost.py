# -*- coding: utf-8 -*-


import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import joblib
import warnings
warnings.filterwarnings('ignore')

# ==================== 1. å‚æ•°è®¾ç½® ====================
CSV_FILE_PATH = 'data/train_all.csv'  # âœ… ä¿®æ”¹ä¸ºä½ çš„ CSV æ–‡ä»¶è·¯å¾„
MODEL_SAVE_PATH = 'models/xgboost_rock_model.pkl'  # æ¨¡å‹ä¿å­˜è·¯å¾„
TEST_SIZE = 0.2
RANDOM_STATE = 42

# ==================== 2. åŠ è½½æ•°æ® ====================
print("ğŸ” æ­£åœ¨åŠ è½½æ•°æ®...")
try:
    data = pd.read_csv(CSV_FILE_PATH)
except FileNotFoundError:
    raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶ï¼š{CSV_FILE_PATH}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")

# æ£€æŸ¥å¿…è¦åˆ—
required_columns = ['DEPTH', 'SP', 'GR', 'AC', 'label']
missing_cols = [col for col in required_columns if col not in data.columns]
if missing_cols:
    raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—ï¼š{missing_cols}")

# æå–ç‰¹å¾å’Œæ ‡ç­¾
X = data[['DEPTH', 'SP', 'GR', 'AC']].copy()
y = data['label'].copy()

print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
print(f"æ ‡ç­¾åˆ†å¸ƒ:\n{y.value_counts().sort_index()}")

# ==================== 3. æ•°æ®é¢„å¤„ç† ====================
print("ğŸ§¹ æ•°æ®é¢„å¤„ç†...")

# å»é™¤ label ä¸º NaN çš„è¡Œ
valid_mask = y.notna()
X = X[valid_mask]
y = y[valid_mask].astype(int)

# å»é™¤ç‰¹å¾ä¸­çš„å¼‚å¸¸å€¼ï¼ˆå¯é€‰ï¼šä½¿ç”¨ 3Ïƒ åŸåˆ™ï¼‰
def remove_outliers(X_df, y_series, threshold=3):
    z_scores = np.abs((X_df - X_df.mean()) / X_df.std())
    no_outlier = (z_scores < threshold).all(axis=1)
    return X_df[no_outlier], y_series[no_outlier]

X, y = remove_outliers(X, y)
print(f"å»é™¤å¼‚å¸¸å€¼åæ•°æ®å½¢çŠ¶: {X.shape}")

# ï¼ˆå¯é€‰ï¼‰æ ‡å‡†åŒ–ï¼šXGBoost ä¸éœ€è¦ï¼Œä½†è¿™é‡Œæä¾›å¼€å…³
USE_SCALER = False
if USE_SCALER:
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    print("å·²å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ã€‚")
else:
    X = X.values  # è½¬ä¸º numpy æ•°ç»„

y = y.values

# ==================== 4. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† ====================
print("âœ‚ï¸  åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=TEST_SIZE,
    random_state=RANDOM_STATE,
    stratify=y  # åˆ†å±‚æŠ½æ ·ï¼Œä¿æŒç±»åˆ«æ¯”ä¾‹
)

print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
print(f"ç±»åˆ«æ•°é‡: {np.unique(y)}")

# ==================== 5. è®­ç»ƒ XGBoost æ¨¡å‹ï¼ˆå¸¦è°ƒå‚ï¼‰ ====================
print("ğŸš€ è®­ç»ƒ XGBoost æ¨¡å‹...")

# åŸºç¡€æ¨¡å‹
base_model = xgb.XGBClassifier(
    objective='multi:softprob',
    eval_metric='mlogloss',
    num_class=len(np.unique(y)),
    use_label_encoder=False,
    random_state=RANDOM_STATE
)

# å‚æ•°ç½‘æ ¼ï¼ˆå¯ç¼©å°æœç´¢èŒƒå›´åŠ å¿«é€Ÿåº¦ï¼‰
param_grid = {
    'n_estimators': [100, 150],
    'max_depth': [4, 5, 6],
    'learning_rate': [0.05, 0.1, 0.15],
    'reg_lambda': [0.5, 1.0, 1.5],
    'gamma': [0.0, 0.1]
}

# ç½‘æ ¼æœç´¢ï¼ˆä½¿ç”¨ Macro-F1 ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼‰
grid_search = GridSearchCV(
    estimator=base_model,
    param_grid=param_grid,
    scoring='f1_macro',
    cv=3,
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

# æœ€ä¼˜æ¨¡å‹
best_model = grid_search.best_estimator_
print(f"âœ… æœ€ä¼˜å‚æ•°: {grid_search.best_params_}")

# ==================== 6. é¢„æµ‹ä¸è¯„ä¼° ====================
print("ğŸ“Š æ­£åœ¨è¯„ä¼°æ¨¡å‹...")

y_pred = best_model.predict(X_test)

# ä¸‰ç§æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡
accuracy = accuracy_score(y_test, y_pred)
macro_f1 = f1_score(y_test, y_pred, average='macro')
f1_per_class = f1_score(y_test, y_pred, average=None)

print("\nâœ… ä¸‰é¡¹æ ¸å¿ƒè¯„ä¼°ç»“æœï¼š")
print(f"1. å‡†ç¡®ç‡ (Accuracy):       {accuracy:.4f}")
print(f"2. Macro-F1 Score:          {macro_f1:.4f}")
print("3. å„ç±»åˆ« F1 åˆ†æ•°:")
classes = np.unique(y)
for i, cls in enumerate(classes):
    print(f"   ç±»åˆ« {cls}: {f1_per_class[i]:.4f}")

# è¯¦ç»†æŠ¥å‘Š
print("\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Šï¼ˆå« Precision, Recallï¼‰:")
target_names = [f'Rock_{c}' for c in classes]
print(classification_report(y_test, y_pred, target_names=target_names))

# ==================== 7. ç‰¹å¾é‡è¦æ€§å¯è§†åŒ– ====================
print("ğŸ“ˆ ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§...")

importance = best_model.feature_importances_
features = ['DEPTH', 'SP', 'GR', 'AC']

plt.figure(figsize=(8, 5))
indices = np.argsort(importance)[::-1]
plt.bar(range(len(importance)), importance[indices], 
        color=['skyblue', 'lightgreen', 'salmon', 'wheat'][indices[0]],
        edgecolor='black')
plt.xticks(range(len(importance)), [features[i] for i in indices])
plt.title('Feature Importance in XGBoost Model', fontsize=14)
plt.ylabel('Importance (Based on Gain)', fontsize=12)
plt.xlabel('Features', fontsize=12)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

# ==================== 8. ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ ====================
print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {MODEL_SAVE_PATH}...")

# ä¿å­˜æ¨¡å‹ï¼ˆå«é¢„å¤„ç†å™¨ï¼Œå¦‚æœç”¨äº†ï¼‰
model_package = {
    'model': best_model,
    'scaler': scaler if USE_SCALER else None,
    'features': ['DEPTH', 'SP', 'GR', 'AC'],
    'classes': np.unique(y),
    'preprocessing': 'StandardScaler' if USE_SCALER else 'None'
}
joblib.dump(model_package, MODEL_SAVE_PATH)
print("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸï¼")

# ==================== 9. ç¤ºä¾‹ï¼šå¦‚ä½•ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹æ–°æ•°æ® ====================
print("\nğŸ§ª ç¤ºä¾‹ï¼šä½¿ç”¨ä¿å­˜çš„æ¨¡å‹é¢„æµ‹æ–°æ•°æ®...")

# åŠ è½½æ¨¡å‹
loaded = joblib.load(MODEL_SAVE_PATH)
model = loaded['model']
scaler = loaded['scaler']

# æ¨¡æ‹Ÿä¸€æ¡æ–°æµ‹äº•æ•°æ®ï¼ˆDEPTH, SP, GR, ACï¼‰
new_data = np.array([[1250.0, 50.0, 75.0, 280.0]])  # å½¢çŠ¶: (1, 4)

# å¦‚æœè®­ç»ƒæ—¶ç”¨äº†æ ‡å‡†åŒ–ï¼Œé¢„æµ‹æ—¶ä¹Ÿè¦ç”¨
if scaler is not None:
    new_data = scaler.transform(new_data)

# é¢„æµ‹
pred_label = model.predict(new_data)[0]
pred_proba = model.predict_proba(new_data)[0]
print(f"æ–°æ ·æœ¬ç‰¹å¾: DEPTH=1250, SP=50, GR=75, AC=280")
print(f"é¢„æµ‹å²©æ€§ç±»åˆ«: {pred_label}")
print(f"å„ç±»åˆ«æ¦‚ç‡: {dict(zip(loaded['classes'], pred_proba))}")